import Head from "next/head";
import Image from "next/image";
import { Inter } from "next/font/google";
import styles from "@/styles/Home.module.css";
import { useEffect, useRef, useState } from "react";
import { createFFmpeg, fetchFile } from "@ffmpeg/ffmpeg";

const inter = Inter({ subsets: ["latin"] });
const ffmpeg = createFFmpeg({
  log: true,
  corePath: "https://unpkg.com/@ffmpeg/core@0.11.0/dist/ffmpeg-core.js",
});

export default function Home() {
  const [transcription, setTranscription] = useState("");
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder>();
  const [currentFrequency, setCurrentFrequency] = useState(0);
  const [breakFrequency, setBreakFrequency] = useState(0);

  const handleStartRecording = async () => {
    setIsRecording(true);
    const audioContext = new AudioContext();
    const stream = navigator.mediaDevices.getUserMedia({ audio: true });

    // Record the audio using MediaRecorder
    const chunks: BlobPart[] = [];
    const mediaRecorder = new MediaRecorder(await stream);
    mediaRecorderRef.current = mediaRecorder;

    // Create analyzer node to get audio data
    const analyzer = audioContext.createAnalyser();
    analyzer.fftSize = 2048;
    const bufferLength = analyzer.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const sampleRate = audioContext.sampleRate;

    // Connect the audio stream to the analyzer node
    const source = audioContext.createMediaStreamSource(await stream);
    source.connect(analyzer);

    // Analyze the audio data and send it to the OpenAI API
    let interval = setInterval(
      async (chunk) => {
        analyzer.getByteFrequencyData(dataArray);
        const avg = dataArray.reduce((acc, val) => acc + val) / bufferLength;
        const decibels = 20 * Math.log10(avg);
        if (decibels < 20) {
          // send the wave data to the API
          const audioBlob = new Blob(chunks, { type: "audio/webm" });
          // check if the audio is long enough to send to the API
          console.log("Sending audio to API...", chunk.length);
          // let response = await transcribeAudio(audioBlob);
          // let newTranscription = transcription + response;
          // setTranscription(newTranscription);

          // delete the audio data
          // chunks.splice(0, chunks.length);
        }
      },
      2000,
      chunks
    );

    mediaRecorder.addEventListener("dataavailable", (e) => {
      console.log("Recording...", chunks);
      chunks.push(e.data);
    });

    mediaRecorder.addEventListener("stop", async () => {
      clearInterval(interval);
      // (await stream).getTracks().forEach((track) => track.stop());
      // const audioBlob = new Blob(chunks, { type: "audio/webm" });
      // const audioBuffer = await audioContext.decodeAudioData(
      //   await audioBlob.arrayBuffer()
      // );
      // console.log("Sending audio to API...", chunks.length);
      // if (!ffmpeg.isLoaded()) await ffmpeg.load();

      // // Convert audio buffer to mp3 using FFmpeg.js
      // ffmpeg.FS("writeFile", "audio.wav", await fetchFile(audioBlob));
      // await ffmpeg.run(
      //   "-i",
      //   "audio.wav",
      //   "-codec:a",
      //   "libmp3lame",
      //   "-qscale:a",
      //   "2",
      //   "audio.mp3"
      // );
      // const mp3Blob = ffmpeg.FS("readFile", "audio.mp3");

      // setTranscription(await transcribeAudio(mp3Blob));
    });
    mediaRecorder.start(500);
  };

  const handleStopRecording = () => {
    setIsRecording(false);
    mediaRecorderRef.current?.stop();
  };

  const transcribeAudio = async (audioBlob: BlobPart) => {
    // Transcribe the audio using the OpenAI API
    const formData = new FormData();
    formData.append("model", "whisper-1");
    formData.append("file", new File([audioBlob], "audio.mp3"));
    const response = await fetch(
      "https://api.openai.com/v1/audio/transcriptions",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer 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`,
        },
        body: formData,
      }
    );
    const json = await response.json();
    console.log(json);
    return json.text;
  };
  return (
    <>
      <Head>
        <title>Create Next App</title>
        <meta name="description" content="Generated by create next app" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="icon" href="/favicon.ico" />
      </Head>
      <main className={styles.main}>
        <div>
          {isRecording ? (
            <button onClick={handleStopRecording}>Stop Recording</button>
          ) : (
            <button onClick={handleStartRecording}>Start Recording</button>
          )}
          <div>Current Frequency : {currentFrequency}</div>
          <div>Break Frequency : {breakFrequency}</div>
          <div>{transcription}</div>
        </div>
      </main>
    </>
  );
}
