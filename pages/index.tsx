import Head from "next/head";
import Image from "next/image";
import { Inter } from "next/font/google";
import styles from "@/styles/Home.module.css";
import { useEffect, useRef, useState } from "react";
import { createFFmpeg, fetchFile } from "@ffmpeg/ffmpeg";

const inter = Inter({ subsets: ["latin"] });
const ffmpeg = createFFmpeg({
  log: true,
  corePath: "https://unpkg.com/@ffmpeg/core@0.11.0/dist/ffmpeg-core.js",
});

export default function Home() {
  const [transcription, setTranscription] = useState("");
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder>();

  const handleStartRecording = async () => {
    setIsRecording(true);
    const audioContext = new AudioContext();
    const stream = navigator.mediaDevices.getUserMedia({ audio: true });

    // Record the audio using MediaRecorder
    const chunks: BlobPart[] = [];
    const mediaRecorder = new MediaRecorder(await stream);
    mediaRecorderRef.current = mediaRecorder;
    mediaRecorder.addEventListener("dataavailable", (e) => chunks.push(e.data));
    mediaRecorder.addEventListener("stop", async () => {
      (await stream).getTracks().forEach((track) => track.stop());
      const audioBlob = new Blob(chunks, { type: "audio/webm" });
      const audioBuffer = await audioContext.decodeAudioData(
        await audioBlob.arrayBuffer()
      );
      if (!ffmpeg.isLoaded()) await ffmpeg.load();

      // Convert audio buffer to mp3 using FFmpeg.js
      ffmpeg.FS("writeFile", "audio.wav", await fetchFile(audioBlob));
      await ffmpeg.run(
        "-i",
        "audio.wav",
        "-codec:a",
        "libmp3lame",
        "-qscale:a",
        "2",
        "audio.mp3"
      );
      const mp3Blob = ffmpeg.FS("readFile", "audio.mp3");

      setTranscription(await transcribeAudio(mp3Blob));
    });
    mediaRecorder.start();
  };

  const handleStopRecording = () => {
    setIsRecording(false);
    mediaRecorderRef.current?.stop();
  };

  const transcribeAudio = async (audioBlob: BlobPart) => {
    // Transcribe the audio using the OpenAI API
    const formData = new FormData();
    formData.append("model", "whisper-1");
    formData.append("file", new File([audioBlob], "audio.mp3"));
    const response = await fetch(
      "https://api.openai.com/v1/audio/transcriptions",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer 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`,
        },
        body: formData,
      }
    );
    const json = await response.json();
    console.log(json);
    return json.text;
  };
  return (
    <>
      <Head>
        <title>Create Next App</title>
        <meta name="description" content="Generated by create next app" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="icon" href="/favicon.ico" />
      </Head>
      <main className={styles.main}>
        <div>
          {isRecording ? (
            <button onClick={handleStopRecording}>Stop Recording</button>
          ) : (
            <button onClick={handleStartRecording}>Start Recording</button>
          )}
          <div>{transcription}</div>
        </div>
      </main>
    </>
  );
}
